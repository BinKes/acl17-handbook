This paper presents our system submitted for the CoNLL 2017 Shared Task, ``Multilingual Parsing from Raw Text to Universal Dependencies.'' We ran the system for all languages with our own fully pipelined components without relying on re-trained baseline systems. To train the dependency parser, we used only the universal part-of-speech tags and distance between words, and applied deterministic rules to assign dependency labels.  The simple and delexicalized models are suitable for cross-lingual transfer approaches and a universal language model.  Experimental results show that our model performed well in some metrics and leads discussion on topics such as contribution of each component and on syntactic similarities among languages.
