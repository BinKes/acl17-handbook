NLP tasks are often limited by the scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn a richer emotional representations. Through emoji prediction on a dataset of 634 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analysis shows that the diversity of our noisy labels is important for the performance of our model. We release our pretrained model.
