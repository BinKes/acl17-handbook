Probabilistic topic modeling is a tool for semantic analysis of texts widely used during the last decades. Word embeddings have gained their popularity more recently being inspired by achievements in neural networks. In this paper we consider both approaches from the perspective of learning hidden semantic representations of words via matrix factorization techniques. We show that a topic modeling performed over word co-occurrence data is capable of producing state-of-the-art results for word similarity task. Unlike SGNS, the components of obtained word embeddings are interpretable and highly sparse. We impose further requirements by additive regularization approach thus bridging the gap between word embeddings and extensions of topic models.
