Variation in language is ubiquitous, and is particularly evident in newer forms of writing such as social media. Fortunately, variation is not random, but is usually linked to social factors. By exploiting linguistic homophily --- the tendency of socially linked individuals to use language similarly --- it is possible to build models that are more robust to variation. In this paper, we focus on social network structures, which make it possible to generalize sociolinguistic properties from authors in the training set to authors in the test sets, without requiring demographic author metadata. We explore the social information by leveraging author embeddings, training with social relations between authors. We introduce an attention based neural architecture --- the prediction is based on several basis models with emphasis on different regions in the network, where socially connected users share similar attention weights on these models. We are able to improve the overall accuracies of Twitter sentiment analysis and review sentiment analysis by significant margins over competitive prior work.