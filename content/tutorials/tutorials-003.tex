\begin{bio}
  {\bfseries Xiaodan Zhu} is a researcher of National Research Council Canada.
  His research interests include naturalÂ language processing, spoken document
  understanding, and machine learning.

  {\bfseries Edward Grefenstette} is a Franco-American computer scientist
  working as a staff research scientist at DeepMind, following a (short) period
  as the CTO of Dark Blue Labs, which he co-founded.

\end{bio}

\begin{tutorial}
  {Deep Learning for Semantic Composition}
  {tutorial-final-003}
  {\daydateyear, \tutorialmorningtime}
  {\TutLocC}

Learning representation to model the meaning of text has been a core problem in
NLP. The last several years have seen extensive interests on distributional
approaches, in which text spans of different granularities are encoded as
vectors of numerical values. If properly learned, such representation has
showed to achieve the state-of-the-art performance on a wide range of NLP
problems.

In this tutorial, we will cover the fundamentals and the state-of-the-art
research on neural network-based modeling for semantic composition, which
aims to learn distributed representation for different granularities of text,
e.g., phrases, sentences, or even documents, from their sub-component meaning
representation, e.g., word embedding.

\end{tutorial}