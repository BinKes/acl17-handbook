SubmissionNumber#=%=#169
FinalPaperTitle#=%=#Optimizing Differentiable Relaxations of Coreference Evaluation Metrics
ShortPaperTitle#=%=#Optimizing Differentiable Relaxations of Coreference Evaluation Metrics
NumberOfPages#=%=#10
CopyrightSigned#=%=#Phong Le
JobTitle#==#
Organization#==#Phong Le
ILLC, University of Amsterdam
Abstract#==#Coreference evaluation metrics are hard to optimize directly as they are
non-differentiable functions, not easily decomposable into elementary
decisions. Consequently, most approaches optimize objectives only indirectly
related to the end goal, resulting in suboptimal performance. Instead, we
propose a differentiable relaxation that lends itself to gradient-based
optimisation, thus bypassing the need for reinforcement learning or heuristic
modification of cross-entropy. We show that by modifying the training objective
of a competitive neural coreference system, we obtain a substantial gain in
performance. This suggests that our approach can be regarded as a viable
alternative to using reinforcement learning or more computationally expensive
imitation learning.
Author{1}{Firstname}#=%=#Phong
Author{1}{Lastname}#=%=#Le
Author{1}{Email}#=%=#lephong.xyz@gmail.com
Author{1}{Affiliation}#=%=#University of Amsterdam
Author{2}{Firstname}#=%=#Ivan
Author{2}{Lastname}#=%=#Titov
Author{2}{Email}#=%=#titov@uva.nl
Author{2}{Affiliation}#=%=#University of Edinburgh / University of Amsterdam

==========