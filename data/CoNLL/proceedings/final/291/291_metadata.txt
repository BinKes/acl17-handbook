SubmissionNumber#=%=#291
FinalPaperTitle#=%=#Should Neural Network Architecture Reflect Linguistic Structure?
ShortPaperTitle#=%=#Should Neural Network Architecture Reflect Linguistic Structure?
NumberOfPages#=%=#1
CopyrightSigned#=%=#Julia Hockenmaier (as proxy for Chris Dyer)
JobTitle#==#Ad-hoc CoNLL publication chair
Organization#==#SIGNLL
Abstract#==#I explore the hypothesis that conventional neural network models (e.g.,
recurrent neural networks) are incorrectly biased for making linguistically
sensible generalizations when learning, and that a better class of models is
based on architectures that reflect hierarchical structures for which
considerable behavioral evidence exists. I focus on the problem of modeling and
representing the meanings of sentences. On the generation front, I introduce
recurrent neural network grammars (RNNGs), a joint, generative model of
phrase-structure trees and sentences. RNNGs operate via a recursive syntactic
process reminiscent of probabilistic context-free grammar generation, but
decisions are parameterized using RNNs that condition on the entire (top-down,
left-to-right) syntactic derivation history, thus relaxing context-free
independence assumptions, while retaining a bias toward explaining decisions
via "syntactically local" conditioning contexts. Experiments show that RNNGs
obtain better results in generating language than models that donâ€™t exploit
linguistic structure. On the representation front, I explore unsupervised
learning of syntactic structures based on distant semantic supervision using a
reinforcement-learning algorithm. The learner seeks a syntactic structure that
provides a compositional architecture that produces a good representation for a
downstream semantic task. Although the inferred structures are quite different
from traditional syntactic analyses, the performance on the downstream tasks
surpasses that of systems that use sequential RNNs and tree-structured RNNs
based on treebank dependencies. This is joint work with Adhi Kuncoro, Dani
Yogatama, Miguel Ballesteros, Phil Blunsom, Ed Grefenstette, Wang Ling, and
Noah A. Smith.
Author{1}{Firstname}#=%=#Chris
Author{1}{Lastname}#=%=#Dyer
Author{1}{Email}#=%=#cdyer@google.com
Author{1}{Affiliation}#=%=#Google DeepMind

==========