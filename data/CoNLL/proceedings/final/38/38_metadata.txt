SubmissionNumber#=%=#38
FinalPaperTitle#=%=#Zero-Shot Relation Extraction via Reading Comprehension
ShortPaperTitle#=%=#Zero-Shot Relation Extraction via Reading Comprehension
NumberOfPages#=%=#10
CopyrightSigned#=%=#Omer Levy
JobTitle#==#
Organization#==#University of Washington, Seattle, WA
Abstract#==#We show that relation extraction can be reduced to answering simple reading
comprehension questions, by associating one or more natural-language questions
with each relation slot. This reduction has several advantages: we can (1)
learn relation-extraction models by extending recent neural
reading-comprehension techniques, (2) build very large training sets for those
models by combining relation-specific crowd-sourced questions with distant
supervision, and even (3) do zero-shot learning by extracting new relation
types that are only specified at test-time, for which we have no labeled
training examples. Experiments on a Wikipedia slot-filling task demonstrate
that the approach can generalize to new questions for known relation types with
high accuracy, and that zero-shot generalization to unseen relation types is
possible, at lower accuracy levels, setting the bar for future work on this
task.
Author{1}{Firstname}#=%=#Omer
Author{1}{Lastname}#=%=#Levy
Author{1}{Email}#=%=#omerlevy@cs.washington.edu
Author{1}{Affiliation}#=%=#University of Washington
Author{2}{Firstname}#=%=#Minjoon
Author{2}{Lastname}#=%=#Seo
Author{2}{Email}#=%=#minjoon@cs.washington.edu
Author{2}{Affiliation}#=%=#University of Washington
Author{3}{Firstname}#=%=#Eunsol
Author{3}{Lastname}#=%=#Choi
Author{3}{Email}#=%=#eunsol@cs.washington.edu
Author{3}{Affiliation}#=%=#University of Washington
Author{4}{Firstname}#=%=#Luke
Author{4}{Lastname}#=%=#Zettlemoyer
Author{4}{Email}#=%=#lsz@cs.washington.edu
Author{4}{Affiliation}#=%=#University of Washington

==========