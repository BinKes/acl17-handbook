SubmissionNumber#=%=#17
FinalPaperTitle#=%=#Beam Search Strategies for Neural Machine Translation
ShortPaperTitle#=%=#Beam Search Strategies for Neural Machine Translation
NumberOfPages#=%=#5
CopyrightSigned#=%=#Markus Freitag
JobTitle#==#
Organization#==#IBM
1101 Kitchawan Rd, Yorktown Heights, NY 10598
Abstract#==#The basic concept in Neural Machine Translation (NMT) is to train a large
Neural Network that maximizes the translation performance on a given parallel
corpus. NMT is then using a simple left-to-right beam-search decoder to
generate new translations that approximately maximize the trained conditional
probability. The current beam search strategy generates the target sentence
word by word from left-to-right while keeping a fixed amount of active
candidates at each time step. First, this simple search is less adaptive as it
also expands candidates whose scores are much worse than the current best.
Secondly, it does not expand hypotheses if they are not within the best scoring
candidates, even if their scores are close to the best one. The latter one can
be avoided by increasing the beam size until no performance improvement can be
observed. While you can reach better performance, this has the drawback of a
slower decoding speed. In this paper, we concentrate on speeding up the decoder
by applying a more flexible beam search strategy whose candidate size may vary
at each time step depending on the candidate scores. We speed up the original
decoder by up to 43% for the two language pairs German to English and Chinese
to English without losing any translation quality.
Author{1}{Firstname}#=%=#Markus
Author{1}{Lastname}#=%=#Freitag
Author{1}{Email}#=%=#freitagmarkus@gmx.de
Author{1}{Affiliation}#=%=#IBM Research
Author{2}{Firstname}#=%=#Yaser
Author{2}{Lastname}#=%=#Al-Onaizan
Author{2}{Email}#=%=#onaizan@us.ibm.com
Author{2}{Affiliation}#=%=#IBM T.J. Watson Research Center

==========