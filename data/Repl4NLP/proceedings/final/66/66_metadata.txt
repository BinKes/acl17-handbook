SubmissionNumber#=%=#66
FinalPaperTitle#=%=#Learning to Embed Words in Context for Syntactic Tasks
ShortPaperTitle#=%=#Learning to Embed Words in Context for Syntactic Tasks
NumberOfPages#=%=#11
CopyrightSigned#=%=#Lifu Tu
JobTitle#==#
Organization#==#Toyota Technological Institute at Chicago
6045 South Kenwood Ave
Chicago, IL 60637
Abstract#==#We present models for embedding words in the context of surrounding words. 
Such models, which we refer to as token embeddings, represent the
characteristics of a word that are specific to a given context, such as word
sense, syntactic category, and semantic role. We explore simple, efficient
token embedding models based on standard neural network architectures. We learn
token embeddings on a large amount of unannotated text and evaluate them as
features for part-of-speech taggers and dependency parsers trained on much
smaller amounts of annotated data.  We find that predictors endowed with token
embeddings consistently outperform baseline predictors across a range of
context window and training set sizes.
Author{1}{Firstname}#=%=#Lifu
Author{1}{Lastname}#=%=#Tu
Author{1}{Email}#=%=#lifu@ttic.edu
Author{1}{Affiliation}#=%=#Toyota Technological Institute at Chicago
Author{2}{Firstname}#=%=#Kevin
Author{2}{Lastname}#=%=#Gimpel
Author{2}{Email}#=%=#kgimpel@ttic.edu
Author{2}{Affiliation}#=%=#Toyota Technological Institute at Chicago
Author{3}{Firstname}#=%=#Karen
Author{3}{Lastname}#=%=#Livescu
Author{3}{Email}#=%=#klivescu@ttic.edu
Author{3}{Affiliation}#=%=#TTI-Chicago

==========