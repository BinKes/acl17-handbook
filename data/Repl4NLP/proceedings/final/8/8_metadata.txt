SubmissionNumber#=%=#8
FinalPaperTitle#=%=#Context encoders as a simple but powerful extension of word2vec
ShortPaperTitle#=%=#Context encoders as a simple but powerful extension of word2vec
NumberOfPages#=%=#5
CopyrightSigned#=%=#Franziska Horn
JobTitle#==#
Organization#==#Technische Universität Berlin
Machine Learning Group
Marchstr. 23
10587 Berlin
Abstract#==#With a strikingly simple architecture and the ability to learn meaningful word
embeddings efficiently from texts containing billions of words, word2vec
remains one of the most popular neural language models used today. However, as
only a single embedding is learned for every word in the vocabulary, the model
fails to optimally represent words with multiple meanings and, additionally, it
is not possible to create embeddings for new (out-of-vocabulary) words on the
spot. Based on an intuitive interpretation of the continuous bag-of-words
(CBOW) word2vec model's negative sampling training objective in terms of
predicting context based similarities, we motivate an extension of the model we
call context encoders (ConEc). By multiplying the matrix of trained word2vec
embeddings with a word's average context vector, out-of-vocabulary (OOV)
embeddings and representations for words with multiple meanings can be created
based on the words' local contexts. The benefits of this approach are
illustrated by using these word embeddings as features in the CoNLL 2003 named
entity recognition (NER) task.
Author{1}{Firstname}#=%=#Franziska
Author{1}{Lastname}#=%=#Horn
Author{1}{Email}#=%=#franziska.horn@campus.tu-berlin.de
Author{1}{Affiliation}#=%=#Technische Universität Berlin

==========