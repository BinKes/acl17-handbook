SubmissionNumber#=%=#145
FinalPaperTitle#=%=#Multimodal Word Distributions
ShortPaperTitle#=%=#Multimodal Word Distributions
NumberOfPages#=%=#12
CopyrightSigned#=%=#Ben Athiwaratkun
JobTitle#==#
Organization#==#Cornell University
Abstract#==#Word embeddings provide point representations of words containing useful
semantic information. 
We introduce multimodal word distributions formed from Gaussian mixtures, for
multiple word meanings, entailment, and rich uncertainty information.  To learn
these distributions, we propose an energy-based max-margin objective. We show
that the resulting approach captures uniquely  expressive semantic information,
and outperforms alternatives, such as word2vec skip-grams, and Gaussian
embeddings, on benchmark datasets such as word similarity and entailment.
Author{1}{Firstname}#=%=#Ben
Author{1}{Lastname}#=%=#Athiwaratkun
Author{1}{Email}#=%=#pa338@cornell.edu
Author{1}{Affiliation}#=%=#Cornell University
Author{2}{Firstname}#=%=#Andrew
Author{2}{Lastname}#=%=#Wilson
Author{2}{Email}#=%=#aglwilson@gmail.com
Author{2}{Affiliation}#=%=#Cornell University

==========