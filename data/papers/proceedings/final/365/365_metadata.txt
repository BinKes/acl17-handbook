SubmissionNumber#=%=#365
FinalPaperTitle#=%=#Learning attention for historical text normalization by learning to pronounce
ShortPaperTitle#=%=#Learning attention for historical text normalization by learning to pronounce
NumberOfPages#=%=#13
CopyrightSigned#=%=#Marcel Bollmann
JobTitle#==#
Organization#==#
Abstract#==#Automated processing of historical texts often relies on pre-normalization to
modern word forms. Training encoder-decoder architectures to solve such
problems typically requires a lot of training data, which is not available for
the named task. We address this problem by using several novel encoder-decoder
architectures, including a multi-task learning (MTL) architecture using a
grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art
by an absolute 2% increase in performance. We analyze the induced models across
44 different texts from Early New High German. Interestingly, we observe that,
as previously conjectured, multi-task learning can learn to focus attention
during decoding, in ways remarkably similar to recently proposed attention
mechanisms. This, we believe, is an important step toward understanding how MTL
works.
Author{1}{Firstname}#=%=#Marcel
Author{1}{Lastname}#=%=#Bollmann
Author{1}{Email}#=%=#bollmann@linguistics.rub.de
Author{1}{Affiliation}#=%=#Department of Linguistics, Ruhr-Universität Bochum
Author{2}{Firstname}#=%=#Joachim
Author{2}{Lastname}#=%=#Bingel
Author{2}{Email}#=%=#joabingel@gmail.com
Author{2}{Affiliation}#=%=#University of Copenhagen
Author{3}{Firstname}#=%=#Anders
Author{3}{Lastname}#=%=#Søgaard
Author{3}{Email}#=%=#soegaard@di.ku.dk
Author{3}{Affiliation}#=%=#University of Copenhagen

==========