SubmissionNumber#=%=#467
FinalPaperTitle#=%=#Learning bilingual word embeddings with (almost) no bilingual data
ShortPaperTitle#=%=#Learning bilingual word embeddings with (almost) no bilingual data
NumberOfPages#=%=#12
CopyrightSigned#=%=#Mikel Artetxe
JobTitle#==#
Organization#==#IXA NLP group, University of the Basque Country (UPV/EHU)
Manuel Lardizabal Pasealekua, 1 - 20018 Donostia-San Sebasti√°n, Spain
Abstract#==#Most methods to learn bilingual word embeddings rely on large parallel corpora,
which is difficult to obtain for most language pairs. This has motivated an
active research line to relax this requirement, with methods that use
document-aligned corpora or bilingual dictionaries of a few thousand words
instead. In this work, we further reduce the need of bilingual resources using
a very simple self-learning approach that can be combined with any
dictionary-based mapping technique. Our method exploits the structural
similarity of embedding spaces, and works with as little bilingual evidence as
a 25 word dictionary or even an automatically generated list of numerals,
obtaining results comparable to those of systems that use richer resources.
Author{1}{Firstname}#=%=#Mikel
Author{1}{Lastname}#=%=#Artetxe
Author{1}{Email}#=%=#mikel.artetxe@ehu.eus
Author{1}{Affiliation}#=%=#University of the Basque Country (UPV/EHU)
Author{2}{Firstname}#=%=#Gorka
Author{2}{Lastname}#=%=#Labaka
Author{2}{Email}#=%=#gorka.labaka@ehu.eus
Author{2}{Affiliation}#=%=#University of the Basque Country (UPV/EHU)
Author{3}{Firstname}#=%=#Eneko
Author{3}{Lastname}#=%=#Agirre
Author{3}{Email}#=%=#e.agirre@ehu.eus
Author{3}{Affiliation}#=%=#University of the Basque Country (UPV/EHU)

==========