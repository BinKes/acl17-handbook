SubmissionNumber#=%=#561
FinalPaperTitle#=%=#Semi-supervised sequence tagging with bidirectional language models
ShortPaperTitle#=%=#Semi-supervised sequence tagging with bidirectional language models
NumberOfPages#=%=#10
CopyrightSigned#=%=#Matthew Peters
JobTitle#==#
Organization#==#Allen Institute for Artificial Intelligence
2157 N Northlake Way Suite 110, Seattle, WA 98103
Abstract#==#Pre-trained word embeddings learned from unlabeled text have become a stan-
dard component of neural network archi- tectures for NLP tasks. However, in
most cases, the recurrent network that oper- ates on word-level representations
to pro- duce context sensitive representations is trained on relatively little
labeled data. In this paper, we demonstrate a general semi-supervised approach
for adding pre- trained context embeddings from bidi- rectional language models
to NLP sys- tems and apply it to sequence labeling tasks. We evaluate our model
on two stan- dard datasets for named entity recognition (NER) and chunking, and
in both cases achieve state of the art results, surpassing previous systems
that use other forms of transfer or joint learning with additional labeled data
and task specific gazetteers.
Author{1}{Firstname}#=%=#Matthew
Author{1}{Lastname}#=%=#Peters
Author{1}{Email}#=%=#matthewp@allenai.org
Author{1}{Affiliation}#=%=#Allen Institute for Artificial Intelligence
Author{2}{Firstname}#=%=#Waleed
Author{2}{Lastname}#=%=#Ammar
Author{2}{Email}#=%=#waleed.ammar@gmail.com
Author{2}{Affiliation}#=%=#Allen Institute for Artificial Intelligence
Author{3}{Firstname}#=%=#Chandra
Author{3}{Lastname}#=%=#Bhagavatula
Author{3}{Email}#=%=#chandrab@allenai.org
Author{3}{Affiliation}#=%=#Allen Institute for Artificial Intelligence
Author{4}{Firstname}#=%=#Russell
Author{4}{Lastname}#=%=#Power
Author{4}{Email}#=%=#russellp@allenai.org
Author{4}{Affiliation}#=%=#Allen Institute for Artificial Intelligence

==========