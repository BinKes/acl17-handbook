SubmissionNumber#=%=#590
FinalPaperTitle#=%=#Multi-Task Video Captioning with Video and Entailment Generation
ShortPaperTitle#=%=#Multi-Task Video Captioning with Video and Entailment Generation
NumberOfPages#=%=#11
CopyrightSigned#=%=#Ramakath Pasunuru
JobTitle#==#
Organization#==#
Abstract#==#Video captioning, the task of describing the content of a video, has seen some
promising improvements in recent years with sequence-to-sequence models, but
accurately learning the temporal and logical dynamics involved in the task
still remains a challenge, especially given the lack of sufficient annotated
data. We improve video captioning by sharing knowledge with two related
directed-generation tasks: a temporally-directed unsupervised video prediction
task to learn richer context-aware video encoder representations, and a
logically-directed language entailment generation task to learn better
video-entailing caption decoder representations. For this, we present a
many-to-many multi-task learning model that shares parameters across the
encoders and decoders of the three tasks. We achieve significant improvements
and the new state-of-the-art on several standard video captioning datasets
using diverse automatic and human evaluations. We also show mutual multi-task
improvements on the entailment generation task.
Author{1}{Firstname}#=%=#Ramakanth
Author{1}{Lastname}#=%=#Pasunuru
Author{1}{Email}#=%=#ram@cs.unc.edu
Author{1}{Affiliation}#=%=#UNC Chapel Hill
Author{2}{Firstname}#=%=#Mohit
Author{2}{Lastname}#=%=#Bansal
Author{2}{Email}#=%=#mbansal@cs.unc.edu
Author{2}{Affiliation}#=%=#University of North Carolina at Chapel Hill

==========