SubmissionNumber#=%=#684
FinalPaperTitle#=%=#Gated-Attention Readers for Text Comprehension
ShortPaperTitle#=%=#GA Reader for Text Comprehension
NumberOfPages#=%=#15
CopyrightSigned#=%=#Bhuwan Dhingra
JobTitle#==#
Organization#==#Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA-15213, USA
Abstract#==#In this paper we study the problem of answering cloze-style questions over
documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop
architecture with a novel attention mechanism, which is based on multiplicative
interactions between the query embedding and the intermediate states of a
recurrent neural network document reader. This enables the reader to build
query-specific representations of tokens in the document for accurate answer
selection. The GA Reader obtains state-of-the-art results on three benchmarks
for this task--the CNN \& Daily Mail news stories and the Who Did What dataset.
The effectiveness of multiplicative interaction is demonstrated by an ablation
study, and by comparing to alternative compositional operators for implementing
the gated-attention.
Author{1}{Firstname}#=%=#Bhuwan
Author{1}{Lastname}#=%=#Dhingra
Author{1}{Email}#=%=#bdhingra@andrew.cmu.edu
Author{1}{Affiliation}#=%=#Carnegie Mellon University
Author{2}{Firstname}#=%=#Hanxiao
Author{2}{Lastname}#=%=#Liu
Author{2}{Email}#=%=#hanxiaol@cs.cmu.edu
Author{2}{Affiliation}#=%=#Carnegie Mellon University
Author{3}{Firstname}#=%=#Zhilin
Author{3}{Lastname}#=%=#Yang
Author{3}{Email}#=%=#zhiliny@cs.cmu.edu
Author{3}{Affiliation}#=%=#Carnegie Mellon University
Author{4}{Firstname}#=%=#William
Author{4}{Lastname}#=%=#Cohen
Author{4}{Email}#=%=#wcohen@cs.cmu.edu
Author{4}{Affiliation}#=%=#Carnegie Mellon University
Author{5}{Firstname}#=%=#Ruslan
Author{5}{Lastname}#=%=#Salakhutdinov
Author{5}{Email}#=%=#rsalakhu@cs.cmu.edu
Author{5}{Affiliation}#=%=#Carnegie Mellon University

==========