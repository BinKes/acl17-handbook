SubmissionNumber#=%=#728
FinalPaperTitle#=%=#Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings
ShortPaperTitle#=%=#Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings
NumberOfPages#=%=#11
CopyrightSigned#=%=#John Wieting
JobTitle#==#
Organization#==#
Abstract#==#We consider the problem of learning general-purpose, paraphrastic sentence
embeddings, revisiting the setting of Wieting et al. (2016b). While they found
LSTM recurrent networks to underperform word averaging, we present several
developments that together produce the opposite conclusion. These include
training on sentence pairs rather than phrase pairs, averaging states to
represent sequences, and regularizing aggressively. These improve LSTMs in both
transfer learning and supervised settings. We also introduce a new recurrent
architecture, the Gated Recurrent Averaging Network, that is inspired by
averaging and LSTMs while outperforming them both. We analyze our learned
models, finding evidence of preferences for particular parts of speech and
dependency relations.
Author{1}{Firstname}#=%=#John
Author{1}{Lastname}#=%=#Wieting
Author{1}{Email}#=%=#wieting2@illinois.edu
Author{1}{Affiliation}#=%=#University of Illinois; TTI-Chicago
Author{2}{Firstname}#=%=#Kevin
Author{2}{Lastname}#=%=#Gimpel
Author{2}{Email}#=%=#kgimpel@ttic.edu
Author{2}{Affiliation}#=%=#Toyota Technological Institute at Chicago

==========