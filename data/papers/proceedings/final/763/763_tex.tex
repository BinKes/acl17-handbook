\documentclass[11pt]{article}
\usepackage{acl2017}  

\usepackage{times}
\usepackage{appendix}
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{bigdelim}
\usepackage{microtype}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}

\aclfinalcopy
\def\aclpaperid{763}

\usepackage{color}

\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}

\usepackage[disable]{todonotes}   
\newcommand{\fixme}[2][]{\todo[color=yellow,size=\scriptsize,fancyline,caption={},#1]{#2}} 
\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} 
\newcommand{\jason}[2][]{\note[#1]{jason}{green!40}{#2}}
\newcommand{\nick}[2][]{\note[#1]{nick}{orange!40}{#2}}
\newcommand{\notewho}[3][]{\note[#1]{#2}{blue!40}{#3}}     
\newcommand{\Fixme}[2][]{\noindent}
\newcommand{\Notewho}[3][]{\notewho[inline,#1]{#2}{#3}\noindent}
\newcommand{\Jason}[2][]{\noindent}
\newcommand{\Nick}[2][]{\noindent}

\usepackage{appendix}
\usepackage{multirow}

\usepackage{booktabs}

\usepackage{pifont}
\usepackage{xspace}
\newcommand{\circone}{\ding{172}\xspace}
\newcommand{\circtwo}{\ding{173}\xspace}
\newcommand{\circthree}{\ding{174}\xspace}
\newcommand{\circfour}{\ding{175}\xspace}

\usepackage{tabu}
\newcommand{\att}[1]{\textsc{#1}}

\usepackage{ragged2e}
\usepackage{etoolbox}
\apptocmd{\thebibliography}{\RaggedRight}{}{}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand*\cbottomrule[1]{\cmidrule[\heavyrulewidth]{#1}\addlinespace}
\newcommand*\ctoprule[1]{\addlinespace\cmidrule[\heavyrulewidth]{#1}}

\newcommand{\numfolds}{5}
\newcommand{\numtypes}{50}

\usepackage{tabularx}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{graphics}

\usepackage{tikz}
\usetikzlibrary{backgrounds,positioning,shapes,arrows,calc,automata,fit}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\pr}[1]{p(#1)}
\newcommand{\prapprox}[1]{\tilde{p}(#1)}
\newcommand{\prs}[2]{p_{#1}(#2)}
\newcommand{\xseq}{\mathbf{x}}
\newcommand{\yseq}{\mathbf{y}}
\newcommand{\zseq}{\mathbf{z}}
\newcommand{\BOS}{\#}
\newcommand{\EOS}{\$}
\newcommand{\delim}{\textvisiblespace}
\newcommand{\bern}[1]{\text{Bernouilli}(#1)}
\newcommand{\prmstop}{\theta_{\text{stop}}}
\newcommand{\dpbase}{p_0}
\newcommand{\inclik}{\gamma}

\title{Bayesian Modeling of Lexical Resources for Low-Resource Settings}
\author{Nicholas Andrews \and Mark Dredze \and Benjamin Van Durme \and Jason Eisner \\
  Department of Computer Science and Human Language Technology Center of Excellence \\
  Johns Hopkins University \\
  3400 N. Charles St., Baltimore, MD 21218 USA \\
  {\tt \{noa,eisner,mdredze,vandurme\}@jhu.edu}
}

\begin{document}
\maketitle

\begin{abstract}
Lexical resources such as dictionaries and gazetteers are often used
as auxiliary data for tasks such as part-of-speech induction and named-entity
recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better.
In this paper, we investigate a more robust approach: we stipulate
that the lexicon is the result of an assumed generative
process. Practically, this means that we may treat the lexical
resources as \emph{observations} under the proposed generative model.
The lexical resources provide training data for the generative model
without requiring separate data to estimate lexical feature
weights. We evaluate the proposed approach in two settings:
part-of-speech induction and low-resource named-entity recognition.
\end{abstract}

\section{Introduction}\label{sec:intro}

Dictionaries and gazetteers are useful in many natural language
processing tasks.
These lexical resources may be derived
from freely available sources (such as Wikidata and Wiktionary)
or constructed for a particular domain.
Lexical resources are typically used to complement existing annotations for a given task~\cite{Ando2005,Collobert2011}.
In this paper, we focus instead on low-resource settings where task annotations are unavailable or scarce. Specifically, we use lexical resources to guide part-of-speech induction (\S\ref{sec:pos}) and to bootstrap named-entity recognizers in low-resource languages (\S\ref{sec:ner}).   

Given their success, it is perhaps surprising that incorporating
gazetteers or dictionaries into discriminative models (e.g.\
conditional random fields) may sometimes \emph{hurt performance}. This
phenomena is called \emph{weight under-training}, in which lexical
features---which detect whether a name is listed in the dictionary or
gazetteer---are given excessive weight at the expense of other useful
features such as spelling features that would generalize to unlisted
names~\cite{smith2005logarithmic,Sutton2006,Smith2006}. Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here.

Our observation is that Bayesian modeling provides a principled solution.
The lexicon is itself a dataset that was generated by some process.
Practically, this means that lexicon entries (words or phrases) may be
treated as additional observations.  As a result, these entries
provide information about how names are spelled.  The presence of the
lexicon therefore now {\em improves} training of the spelling features,
rather than {\em competing with} the spelling features to help explain
the labeled corpus.

A downside is that generative models are typically less
feature-rich than their globally normalized discriminative
counterparts (e.g. conditional random fields). In designing our
approach---the {\em hierarchical sequence memoizer (HSM)}---we aim to be reasonably expressive while retaining
practically useful inference algorithms. We propose a Bayesian nonparametric model to
serve as a generative distribution responsible for both lexicon and corpus data. The proposed model \emph{memoizes} previously used lexical entries
(words or phrases) but backs off to a character-level distribution when
generating novel types~\cite{teh2006pylm,mochihashi2009}. We
propose an efficient inference algorithm for the proposed model
using particle Gibbs sampling (\S\ref{sec:inference}).  Our
code is available at \url{https://github.com/noa/bayesner}.

\section{Model}\label{sec:model}

Our goal is to fit a model that can automatically annotate text.  We observe a supervised or unsupervised training corpus.  For each label $y$ in the annotation scheme, we also observe a lexicon of strings of type $y$.
For example, in our tagging task (\S\ref{sec:pos}), a dictionary provides us with a list of words
for each part-of-speech tag $y$.  (These lists need not be disjoint.)
For named-entity recognition (NER, \S\ref{sec:ner}), we use a list of words or phrases for each named-entity type $y$ (PER, LOC, ORG, etc.).\footnote{Dictionaries and knowledge bases provide more information than we use in this paper. For instance, Wikidata also provides
   a wealth of attributes and other metadata for each entity $s$. In principle,
   this additional information could also be helpful in estimating $P_y(s)$; we leave this intriguing possibility for future work.}

\subsection{Modeling the lexicon}\label{sec:lexicon}

We may treat the lexicon for type $y$, of size $m_y$, as having been produced by a set of $m_y$ IID draws from an unknown distribution $P_y$ over the words or named entities of type $y$.  It therefore provides some evidence about $P_y$.  We will later assume that $P_y$ is also used when generating mentions of these words or entities in text.
Thanks to this sharing of $P_y$, if $x=\texttt{Washington}$ is listed in the gazetteer of locations ($y=\textsc{loc}$), we can draw the same conclusions as if we had seen a {\sc loc}-labeled instance of {\tt Washington} in a supervised corpus.

Generalizing this a bit, we may suppose that one observation of string $x$ in the lexicon is equivalent to $c$ labeled tokens of $x$ in a corpus, where the constant $c > 0$ is known as a {\em pseudocount}.  In other words, observing a lexicon of $m_y$ distinct types $\{x_1,\ldots,x_{m_y}\}$ is equivalent to observing a labeled {\em pseudocorpus} of $cm_y$ tokens.  Notice that given such an observation, the prior probability of any candidate distribution $P_y$ is reweighted by the likelihood $\frac{(cm_y)!}{(c!)^{m_y}}\cdot (P_y(x_1) P_y(x_2) \cdots P_y(x_{m_y}))^c$.  Therefore, this choice of $P_y$ can have relatively high posterior probability only to the extent that it assigns high probability to all of the lexicon types.

\subsection{Discussion}

We employ the above model because it has reasonable qualitative behavior and because computationally, it allows us to condition on observed lexicons as easily as we condition on observed corpora.  However, we caution that as a generative model of the lexicon, it is deficient, in the sense that it allocates probability mass to events that cannot actually correspond to any lexicon.  After all, drawing $cm_y$ IID tokens from $P_y$ is highly unlikely to result in exactly $c$ tokens of each of $m_y$ different types, and yet a run of our system will always assume that precisely this happened to produce each observed lexicon!  To avoid the deficiency, one could assume that the lexicon was generated by rejection sampling: that is, the gazetteer author repeatedly drew samples of size $cm_y$ from $P_y$ until one was obtained that had this property, and then returned the  set of distinct types in that sample as the lexicon for $y$.  But this is hardly a realistic description of how gazetteers are actually constructed.  Rather, one imagines that the gazetteer author simply harvested a lexicon of frequent types from $P_y$ or from a corpus of tokens generated from $P_y$.  For example, a much better generative story is that the lexicon was constructed as the first $m_y$ distinct types to appear $\geq c$ times in an unbounded sequence of IID draws from $P_y$.  When $c=1$, this is equivalent to modeling the lexicon as $m_y$ draws {\em without replacement} from $P_y$.\footnote{If we assume that $P_y$ was drawn from a Pitman-Yor process prior (as in \S\ref{sec:pyp}) using the stick-breaking method~\cite{pitman-1996}, it is also equivalent to modeling the lexicon as the set of labels of the {\em first} $m_y$ stick segments (which tend to have high probability).}  Unfortunately, draws without replacement are no longer IID or exchangeable: order matters.  It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent {\em sequence} of tokens (or stick segments).  We therefore adopt the simpler deficient model.

A version of our lexicon model (with $c=1$) was previously used by \newcite[Appendix C]{dreyer-eisner-2011}, who observed a list of verb paradigm types rather than word or entity-name types.

\subsection{Prior distribution over $P_y$}\label{sec:pyp}

We assume {\em a priori} that $P_y$ was drawn from a Pitman-Yor process (PYP)~\cite{pitman1997two}. Both the lexicon and the ordinary corpus are observations that provide information about $P_y$. The PYP is defined by three parameters: a concentration parameter $\alpha$, a discount parameter $d$, and a base distribution $H_y$. In our case, $H_y$ is a distribution over $\mathcal{X} = \Sigma^*$, the set of possible strings over a finite character alphabet $\Sigma$.

For example, $H_{\textsc{loc}}$ is used to choose new place names, so
it describes what place names tend to look like in the language.  The
draw $P_{\textsc{loc}} \sim \mathrm{PYP}(d,\alpha,H_{\textsc{loc}})$
is an ``adapted'' version of $H_{\textsc{loc}}$.  It is
$P_{\textsc{loc}}$ that determines how often each name is mentioned in
text (and whether it is mentioned in the lexicon).  Some names such as
$\texttt{Washington}$ that are merely plausible under
$H_{\textsc{loc}}$ are far more frequent under $P_{\textsc{loc}}$,
presumably because they were chosen as the names of actual,
significant places.  These place names were randomly drawn from
$H_{\textsc{loc}}$ as part of the procedure for drawing $P_y$.

The expected value of $P_y$ is $H$ (i.e., $H$ is the mean of the PYP
distribution), but if $\alpha$ and $d$ are small, then a typical draw
of $P_y$ will be rather different from $H$, with much of the
probability mass falling on a subset of the strings.

At training or test time, when deciding whether to label a corpus token of $x=\texttt{Washington}$ as a place or person,
we will be interested in the relative values of $P_{\textsc{loc}}(x)$ and $P_{\textsc{per}}(x)$.  In practice, we do not have to represent the unknown infinite object $P_y$,
but can integrate over its possible values.
When $P_y \sim \mathrm{PYP}(d,\alpha,H_y)$, then
a sequence of draws $X_1,X_2,\ldots \sim P_y$ is distributed according to a Chinese restaurant process,
via
\begin{align}
P_y(X_{i+1}&=x \mid X_1,\ldots,X_i) \label{eqn:crp} \\
  & = \frac{\mathrm{customers}(x) - d\cdot \mathrm{tables}(x)}{\alpha + i} \nonumber \\
  & \qquad +  \frac{\alpha + d \cdot \sum_{x'} \mathrm{tables}(x')}{\alpha + i} H_y(x) \nonumber
\end{align}
where $\mathrm{customers}(x) \leq i$
is the number of times that $x$ appeared among $X_1,\ldots,X_i$, and $\mathrm{tables}(x) \leq \mathrm{customers}(x)$ is the number of those times that $x$ was drawn from $H_y$ (where each $P_y(X_i \mid \cdots)$ defined by \eqref{eqn:crp} is interpreted as a mixture distribution that {\em sometimes} uses $H_y$).

\subsection{Form of the base distribution $H_y$}\label{sec:basedist}

By fitting $H_y$ on corpus and lexicon data, we learn what place names or noun strings tend to look like in the language.  By simultaneously fitting $P_y$, we learn which ones are commonly mentioned.  Recall that under our model, tokens are drawn from $P_y$ but the underlying types are drawn from $H_y$, e.g., $H_y$ is responsible for (at least) the first token of each type.\looseness=-1

A simple choice for $H_y$ is a Markov process
that emits characters in $\Sigma \cup \{\EOS\}$, where $\EOS$
is a distinguished stop symbol that indicates the end of the string.
Thus, the probability of producing $\EOS$ controls the typical string
length under $H_y$.

We use a more sophisticated model of strings---a sequence memoizer (SM), which is a (hierarchical) Bayesian treatment of variable-order Markov modeling~\cite{wood2009}.  The SM allows dependence on an unbounded history, and the probability of a given sequence (string) can be found efficiently much as in equation~\eqref{eqn:crp}.

Given a string $x = a_1 \cdots a_J \in \Sigma^*$, the SM assigns a
probability to it via

\noindent
\begin{align}
\!\!H_y(\bm{a}_{1:J}) &= \Big( \prod_{j=1}^J H_y(a_j \mid \bm{a}_{1:j-1}) \Big) H_y(\EOS \mid \bm{a}_{1:J}) \nonumber \\
& = \Big( \prod_{j=1}^J H_{y,{\bm{a}_{1:j-1}}}(a_j) \Big) H_{y,{\bm{a}_{1:J}}}(\EOS)
\end{align}
where $H_{y,{\bm{u}}}(a)$ denotes the conditional probability of character
$a$ given the left context $\bm{u} \in
\Sigma^{*}$. Each $H_{y,{\mathbf{u}}}$ is a distribution over
$\Sigma$, defined recursively as
\begin{align}\label{eqn:H}
H_{y,\epsilon} & \sim \mathrm{PYP}(d_\epsilon, \alpha_\epsilon, \mathcal{U}_{\,\Sigma}) \\
H_{y,\bm{u}} & \sim \mathrm{PYP}(d_{|\mathbf{u}|}, \alpha_{|\mathbf{u}|}, H_{y,\sigma(\mathbf{u})}) \nonumber
\end{align}
where $\epsilon$ is the empty sequence, $\mathcal{U}_{\,\Sigma}$ is
the uniform distribution over $\Sigma \cup \{\EOS\}$, and $\sigma(\mathbf{u})$ drops the first symbol from
$\mathbf{u}$. The discount and concentration parameters
$(d_{|\mathbf{u}|}, \alpha_{|\mathbf{u}|})$ are associated with the
lengths of the contexts $|\bm{u}|$, and should generally be
larger for longer (more specific) contexts, implying stronger backoff
from those contexts.\footnote{We fix these hyperparameters using the values
  suggested in~\cite{wood2009,gasthaus2010}, which we find to be quite
  robust in practice. One could also resample their
  values~\cite{blunsom2011hierarchical}; we experimented with this but did not observe any consistent advantage to
  doing so in our setting.}

Our inference procedure is largely indifferent to the form of $H_y$, so the SM is not the only option.  It would be possible to inject more assumptions into $H_y$, for instance via structured priors for morphology or a grammar of name structure.
Another possibility is to use a parametric model such as a neural language model (e.g.,\ \newcite{jozefowicz2016exploring}), although this would require an inner-loop of gradient optimization.

\subsection{Modeling the sequence of tags $y$}\label{sec:context}

We now turn to modeling the corpus.
We assume that each sentence is generated via a sequence of latent labels
$\bm{y} = \bm{y}_{1:T} \in \mathcal{Y}^*$.\footnote{The label sequence is terminated by a
  distinguished end-of-sequence label, again written as \EOS.}
The observations $\bm{x}_{1:T}$ are then generated conditioned on the label sequence via the corresponding $P_y$ distribution (defined in~\S\ref{sec:pyp}). All observations with the {\em same} label $y$ are drawn from the same $P_y$, and thus this {\em subsequence} of observations is distributed according to the Chinese restaurant process~\eqref{eqn:crp}.

We model $\bm{y}$ using another sequence memoizer model.  This is similar to other hierarchical Bayesian models of latent sequences~\cite{goldwater.griffiths:fully,blunsom2011hierarchical}, but again, it does not limit the Markov order (the number of preceding labels that are conditioned on).  Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in~\S\ref{sec:basedist}, that is,
\begin{align}\label{eqn:labelseq}
p(\bm{y}_{1:T}) := \Big( \prod_{t=1}^T G_{\bm{y}_{1:t-1}}(y_t) \Big) G_{\bm{y}_{1:T}}(\EOS)
\end{align}
where $G_{\bm{v}}(y)$ denotes the conditional probability of latent label
$y \in \mathcal{Y}$ given the left context $\bm{v} \in \mathcal{Y}^{*}$.  Each $G_{\mathbf{v}}$ is a distribution over $\mathcal{Y}$, defined recursively as
\begin{align}\label{eqn:G}
  G_{\epsilon} & \sim \mathrm{PYP}(d_\epsilon, \alpha_\epsilon, \mathcal{U}_{\mathcal{Y}}) \\
  G_{\bm{v}} & \sim \mathrm{PYP}(d_{|\mathbf{v}|}, \alpha_{|\mathbf{v}|}, G_{\sigma(\mathbf{v})}) \nonumber
\end{align}
The probability of transitioning
to label $y_t$ depends on the assignments of all previous labels
$y_1 \ldots y_{t-1}$.

For part-of-speech induction, each label $y_t$ is the part-of-speech associated with the corresponding word $x_t$. For named-entity recognition, we say that each word token is labeled with a named entity type (\textsc{loc}, \textsc{per}, \ldots),\footnote{In \S\ref{sec:inf2}, we will generalize this labeling scheme to allow multi-word named entities such as \texttt{New York}.}
 or with {\em itself} if it is not a named entity but rather a ``context word.''  For example, the word token $x_t=\texttt{Washington}$ could have been emitted from the label $y_t=\textsc{loc}$, or from $y_t=\textsc{per}$, or from $y_t=\texttt{Washington}$ itself (in which case $p(x_t\mid y_t)=1$).  This uses a much larger set of labels ${\cal Y}$ than in the traditional setup where all context words are emitted from the same latent label type \texttt{O}.  Of course, most labels are impossible at most positions (e.g., $y_t$ cannot be \texttt{Washington} {\em unless} $x_t=\texttt{Washington}$).
This scheme makes our generative model sensitive to specific contexts (which is accomplished in discriminative NER systems by contextual features).  For example, the SM for $\bm{y}$ can learn that \texttt{spoke to \textsc{per} yesterday} is a common 4-gram in the label sequence $\bm{y}$, and thus we are more likely to label \texttt{Washington} as a person if $\bm{x}=\ldots\texttt{spoke to Washington yesterday}\ldots$.

We need one change to make this work, since now $\cal Y$ must include
not only the standard NER labels $\mathcal{Y}' = \{\textsc{per},
\textsc{loc}, \textsc{org}, \textsc{gpe}\}$
but also words like $\texttt{Washington}$.  Indeed, now
$\mathcal{Y} = \mathcal{Y}' \cup \Sigma^*$.  But  
no uniform distribution exists over the infinite set $\Sigma^*$, so how
should we replace the base distribution $\mathcal{U}_{\mathcal{Y}}$
over labels in equation~\eqref{eqn:G}?  Answer: To draw from the new base distribution,
sample $y \sim \mathcal{U}_{\mathcal{Y}' \,\cup\, \{\textsc{context}\}}$.
If $y=\textsc{context}$, however, then ``expand'' it by resampling $y \sim H_{\textsc{context}}$.
Here $H_{\textsc{context}}$ is the base distribution over spellings of context words,
and is learned just like the other $H_y$ distributions in \S\ref{sec:basedist}.

\section{Inference via particle Markov chain Monte Carlo}\label{sec:inference}

\subsection{Sequential sampler}\label{sec:inf1}

Taking $\bm{Y}$ to be a random variable, we are interested in the posterior distribution $p(\bm{Y}=\bm{y} \mid \bm{x})$
over label sequences $\bm{y}$ given the emitted word sequence $\bm{x}$.
Our model does not
admit an efficient dynamic programming algorithm,
owing to the dependencies introduced among the $\bm{Y}_t$ when we marginalize over the unknown $G$ and $P$ distributions that govern transitions and emissions, respectively.
In contrast to tagging with a hidden Markov model tagging, the distribution of each label $Y_t$ depends on {\em all} previous labels $\bm{y}_{1:t-1}$, for two reasons: \circone~The transition distribution $p(Y_t=y \mid \bm{y}_{1:t-1})$ has unbounded dependence because of the PYP prior \eqref{eqn:labelseq}.  \circtwo~The emission distribution $p(x_t \mid Y_t=y)$ depends on the emissions observed from any earlier tokens of $y$, because of the Chinese restaurant process \eqref{eqn:crp}.
When \circtwo is the only
complication, block Metropolis-Hastings samplers have proven
effective~\cite{johnson2007bayesian}. However, this approach uses
dynamic programming to sample from a proposal distribution efficiently, which \circone precludes in our case. Instead, we use sequential Monte Carlo (SMC)---sometimes called \emph{particle filtering}---as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation~\cite{borsch2011}, to make decisions before all of $\bm{x}$ has been observed. However, we are interested in the inference (or \emph{smoothing}) problem that conditions on all of $\bm{x}$ \cite{Dubbin:2012:UBP:2405473.2405539,Tripuraneni:2015:PGI:2969442.2969507}.

SMC employs a {\em proposal distribution} $q(\bm{y} \mid \bm{x})$ whose definition decomposes as follows:
\begin{align}
q(y_1 \mid x_1) \prod_{t=2}^T q(y_t \mid \bm{y}_{1:t-1}, \bm{x}_{1:t})
\end{align}
for $T=|\bm{x}|$.
To sample a sequence of latent labels, first sample an initial label $y_1$ from $q_1$,
then proceed incrementally by sampling $y_t$ from $q_t(\cdot \mid \bm{y}_{1:t-1}, \bm{x}_{1:t})$ for
$t=2,\ldots,T$.  The final sampled sequence $\bm{y}$
is called a {\it
  particle}, and is given an {\em unnormalized importance weight} of $\tilde{w} = \tilde{w}_T \cdot p(\EOS \mid \bm{y}_{1:T})$ where $\tilde{w}_T$ was built up via
\begin{align}\label{eqn:incr}
\tilde{w}_t := \tilde{w}_{t-1} \cdot \frac{p(\bm{y}_{1:t}, \bm{x}_{1:t})}{p(\bm{y}_{1:t-1}, \bm{x}_{1:t-1})\,q(y_t\mid \bm{y}_{1:t-1}, \bm{x}_{1:t})}
\end{align}
The SMC procedure consists of generating a system of
$M$ weighted particles whose unnormalized importance
weights $\tilde{w}^{(m)}: 1 \leq m \leq M$ are normalized into $w^{(m)} := \tilde{w}^{(m)} / \sum_{m=1}^M
\tilde{w}^{(m)}$. As $M \rightarrow
\infty$, SMC provides a consistent estimate of the marginal likelihood
$p(\bm{x})$ as $\frac{1}{M} \sum_{m=1}^M \tilde{w}^{(m)}$,
and samples from the weighted particle system are distributed as
samples from the desired posterior $p(\bm{y}\mid\bm{x})$~\cite{doucet2009tutorial}.

\vspace{5pt}

\noindent {\bf Particle Gibbs.} We employ SMC as a kernel in an MCMC sampler~\cite{andrieu2010particle}. In
particular, we use a block Gibbs sampler in which we iteratively
resample the hidden labeling $\bm{y}$ of a sentence $\bm{x}$
conditioned on the current labelings for all other sentences in the corpus. In this
context, the algorithm is called {\it conditional\/} SMC since one
particle is always fixed to the previous sampler state for the sentence being resampled, which ensures that the MCMC procedure is ergodic. At a high
level, this procedure is analogous to other Gibbs samplers
(e.g.\ for topic models), except that the conditional SMC (CSMC) kernel uses
auxiliary variables (particles) in order to generate the new block
variable assignments.
The procedure is outlined in Algorithm~\ref{alg:pgkernel}. Given a previous latent state assignment $\bm{y}'_{1:T}$ and observations $\bm{x}_{1:T}$, the CSMC kernel produces a new latent state assignment via $M$ auxiliary particles where one particle is fixed to the previous assignment. For ergodicity, $M \geq 2$, where larger values of $M$ may improve mixing rate at the expense of increased computation per step.

\vspace{5pt}

\noindent {\bf Proposal distribution.} The choice of proposal
distribution $q$ is crucial to the performance of SMC methods. In the case
of continuous latent variables, it is common to propose $y_t$ from the transition
probability $p(Y_t \mid \bm{y}_{1:t-1})$ because this distribution usually has
a simple form that permits efficient sampling.  However,
it is possible to do better in the case of discrete latent
variables. The optimal proposal distribution is the one which
minimizes the variance of the importance weights, and is given by
\begin{align}\label{eqn:opt}
q(y_t \mid \bm{y}_{1:t-1}, \bm{x}_{1:t}) &:= p(y_t \mid \bm{y}_{1:t-1}, \bm{x}_{1:t}) \\
&= \frac{p(y_t \mid \bm{y}_{1:t-1}) p(x_t \mid y_t) }{p(x_t \mid \bm{y}_{1:t-1})} \notag
\end{align}
where
\begin{align}\label{eqn:optdenom}
p(x_t \mid \bm{y}_{1:t-1}) \!&=\! \sum_{y_t \in \mathcal{Y}} p(y_t \mid \bm{y}_{1:t-1}) p(x_t \mid y_t)
\end{align}
Substituting this expression in equation~\eqref{eqn:incr} and simplifying yields the
incremental weight update:
\begin{align}\label{eqn:weight}
\tilde{w}_t := \tilde{w}_{t-1} \cdot p(x_t \mid \bm{y}_{1:t-1})
\end{align}

\vspace{5pt}

\noindent {\bf Resampling.} In filtering applications, it is common to
use resampling operations to prevent weight degeneracy. We do not find
resampling necessary here for three reasons. First, note that we
resample hidden label sequences that are only as long as the number of
words in a given sentence. Second, we use a proposal which minimizes
the variance of the weights. Finally, we use SMC as a kernel embedded
in an MCMC sampler; asymptotically, this procedure yields samples from
the desired posterior regardless of degeneracy (which only affects
the mixing rate). Practically speaking, one can diagnose the need for
resampling via the effective sample size (ESS) of the particle
system:
\begin{align}\nonumber
  \text{ESS} := \frac{1}{\sum_{m=1}^M (\tilde{w}^{(m)})^2} = \frac{(\sum^M_{m=1} w^{(m)})^2}{\sum_{m=1}^M (w^{(m)})^2}
\end{align}
In our experiments, we find that ESS remains high (a significant
fraction of $M$) even for long sentences, suggesting that resampling
is not necessary to enable mixing of the the Gibbs sampler.

\begin{algorithm}[t]
   \caption{Conditional SMC}\label{alg:pgkernel}
   \begin{algorithmic}[1] 
     \Procedure{CSMC}{$\bm{x}_{1:T}$, $\bm{y}'_{1:T}$, $M$} 
     \State Draw $y_1^{(m)}$ (eqn.\ \ref{eqn:opt}) for $m \in [1, M-1]$
     \State Set $y_1^{(M)} = y_1'$ 
     \State Set $\tilde{w}_1^{(m)}$ (eqn.\ \ref{eqn:weight}) for $m \in [1, M]$
     \For{$t = 2$ to $T$}
     \State Draw $y_t^{(m)}$ (eqn.\ \ref{eqn:opt}) for $m \in [1, M-1]$
     \State Set $y_t^{M} = y_t'$ 
     \State Set $\tilde{w}_t^{(m)}$ (eqn.\ \ref{eqn:weight}) for $m \in [1, M]$
     \EndFor
     \State Set $\tilde{w}^{(m)} \!=\! \tilde{w}_T^{(m)}p(\EOS|\bm{y}_{1:T})$ for $m \in [1, M]$
     \State Draw index $k$ where $p(k=m) \propto \tilde{w}^{(m)}$
     \State \Return $\bm{y}^{(k)}_{1:T}$
     \EndProcedure
   \end{algorithmic}
\end{algorithm}

\vspace{5pt}

\noindent \textbf{Decoding.} In order to obtain a single latent
variable assignment for evaluation purposes, we simply take the state of the
Markov chain after a fixed number of iterations of particle Gibbs. In
principle, one could collect many samples during particle Gibbs and use them
to perform minimum Bayes risk decoding
under a given loss function.  However, this approach is somewhat slower and did
not appear to improve performance in preliminary experiments

\subsection{Segmental sampler}\label{sec:inf2}

We now present an sampler for settings such as NER where each latent label emits a {\em segment} consisting of 1 or more words.
We make use of the same transition distribution $p(y_t \mid \bm{y}_{1:t-1})$, which determines the probability of a label in a given context, and an emission
distribution $p(x_t \mid y_t)$ (namely $P_{y_t}$); these are assumed to be drawn from hierarchical Pitman-Yor processes described in~\S\ref{sec:context} and~\S\ref{sec:lexicon}, respectively. To allow the $x_t$ to be a multi-word string, we simply augment the character set with a distinguished space symbol $\delim \in \Sigma$ that separates words within a string.  For instance, {\texttt New York} would be generated as the 9-symbol sequence \texttt{New{\delim}York}\EOS.

Although the {\em model} emits \texttt{New{\delim}York} all at once, we still formulate our {\em inference
procedure} as a particle filter that proposes one tag for each word.Thus, for a given segment label type $y$, we allow two tag types for its words:
\begin{itemize}[noitemsep]
\item I-$y$ corresponds to a non-final word in a segment of type $y$ (in effect, a word with a following {\delim} attached).
\item E-$y$ corresponds to the final word in a segment of type $y$.
\end{itemize}
For instance, $\bm{x}_{1:2}=\texttt{New York}$ would be annotated as a location segment by defining $\bm{y}_{1:2}=\text{I-}\textsc{loc} \ \text{E-}\textsc{loc}$.
This says that $\bm{y}_{1:2}$ has {\em jointly} emitted $\bm{x}_{1:2}$, an event with prior probability $P_{\textsc{loc}}(\texttt{New{\delim}York})$.
Each word that is not part of a named entity is considered to be a single-word segment.  For example, if the next word were $x_3=\texttt{hosted}$ then it should be tagged with $y_3=\texttt{hosted}$ as in \S\ref{sec:context}, in which case $x_3$ was emitted with probability 1.

To adapt the sampler described in~\S\ref{sec:inf1} for the segmental case, we need only to define the transition and emission probabilities used in equation~\eqref{eqn:opt} and its denominator~\eqref{eqn:optdenom}.

For the transition probabilities, we want to model the sequence of segment labels.  If $y_{t-1}$ is an I- tag, we take $p(y_t \mid \bm{y}_{1:t-1})=1$ , since then $y_t$ merely continues an existing segment.  Otherwise $y_t$ starts a new segment, and we take $p(y_t \mid \bm{y}_{1:t-1})=1$ to be defined by the PYP's probability $G_{\bm{y}_{1:t-1}}(y_t)$ as usual, but where we interpret the subscript $\bm{y}_{1:t-1}$ to refer to the possibly shorter sequence of segment labels implied by those $t-1$ tags.

For the emission probabilities, if $y_t$ has the form I-$y$ or E-$y$, then its associated emission probability no longer has the form $p(x_t \mid y_t)$, since the choice of $x_t$ also depends on any words emitted earlier in the segment.  Let $s \leq t$ be the starting position of the segment that contains $t$.  If $y_t=\text{E-}y$, then the emission probability is proportional to $P_y(x_s\delim x_{s+1}\delim\ldots\delim x_t)$.  If $y_t=\text{I-}y$ then the emission probability is proportional to the {\em prefix probability} $\sum_x P_y(x)$ where $x$ ranges over all strings in $\Sigma^*$ that have
$x_s\delim x_{s+1}\delim\ldots\delim x_t\delim$ as a proper prefix.  Prefix probabilities in $H_y$ are easy to compute because $H_y$ has the form of a language model, and prefix probabilities in $P_y$ are therefore also easy to compute (using a prefix tree for efficiency).

This concludes the description of the segmental sampler. Note that the particle Gibbs procedure is unchanged.

\section{Inducing parts-of-speech with type-level supervision}\label{sec:pos}

Automatically inducing parts-of-speech from raw text is a challenging
problem~\cite{goldwater2005}. Our focus here is on the easier problem
of type-supervised part-of-speech induction, in which (partial)
dictionaries are used to guide
inference~\cite{garrette2012,li2012wiki}. Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in~\S\ref{sec:inf1} to impute the latent parts-of-speech.

Since dictionaries are freely available for hundreds of
languages,\footnote{\url{https://www.wiktionary.org/}} we see this as a mild additional requirement in practice over the
purely unsupervised setting.

In prior work, dictionaries have
been used as constraints on possible parts-of-speech: words appearing
in the dictionary take one of their known parts-of-speech.  In our setting, however, the dictionaries are not constraints but evidence.  If \texttt{monthly} is listed in (only) the adjective lexicon, this tells us that $P_{\textsc{adj}}$ sometimes generates \texttt{monthly} and therefore that $H_{\textsc{adj}}$ may also tend to generate other words that end with \texttt{-ly}.  However, for us, $P_{\textsc{adv}}(\texttt{monthly}) > 0$ as well, allowing us to still correctly treat \texttt{monthly} as a possible adverb if we later encounter it in a training or test corpus.

\subsection{Experiments}\label{sec:posexpts}

We follow the experimental procedure described in~\newcite{li2012wiki}, and use their released code and data to compare to their best model: a second-order maximum entropy Markov model
parametrized with log-linear features (\textsc{shmm-me}).  This
model uses hand-crafted features designed to distinguish between
different parts-of-speech, and it has special handling for rare words.
This approach is surprisingly effective and outperforms alternate
approaches such as cross-lingual transfer~\cite{das2011}. However, it
also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors.

The lexicons are taken from the Wiktionary project; their size and coverage are
documented by \cite{li2012wiki}. 
We evaluate our model on multi-lingual data released as part of the
CoNLL 2007 and CoNLL-X shared tasks.  In particular, we use the same
set of languages as~\newcite{li2012wiki}.\footnote{With the exception of
Dutch.  Unlike the other CoNLL languages, Dutch includes
  phrases, and the procedure by which these were split into tokens was
  not fully documented.} For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling the states for a each sentence in the corpus. The final sampler state is then taken as a 1-best tagging of the unlabeled data.

  \vspace{5pt}

  \noindent \textbf{Results.} The results are reported in Table~\ref{tab:pos}. We find that our hierarchical sequence memoizer ({\sc HSM}) matches or exceeds the performance of the baseline ({\sc shmm-me}) for nearly all the tested languages, particularly for morphologically rich languages such as German where the spelling distributions $H_y$ may capture regularities. It is interesting to note that our model performs worse relative to the baseline for English; one possible explanation is that the baseline uses hand-engineered features whereas ours does not, and these features may have been tuned using English data for validation. 

  Our generative model is supposed to exploit lexicons well.  To see what is lost from using a generative model, we also compared with \newcite{li2012wiki} on standard supervised tagging without any lexicons. 
  Even here our generative model is very competive, losing only on English and Swedish.  

\begin{table*}\centering
\small
\ra{1.17}
\begin{tabular}{ll*{8}{c}|c} \ctoprule{2-11}
 & {\small \bf Model}		& \small Danish 	& \small German	& \small Greek 	& \small English	& \small Italian	& \small Portuguese	& \small Spanish	& \small Swedish & \textbf{Mean} \\ 
\multirow{2}{*}{\small Wiktionary} & \small \textsc{shmm-me} 	& 83.3          & 85.8         & 79.2        & 87.1 & 86.5        & 84.5        & 86.4        & 86.1   & 84.9 \\
   & \small \textsc{hsm}                                                                 & 83.7  & 90.7 & 81.7 & 84.0 & 86.7 & 85.5 & 87.6 & 86.8 & 85.8  \\ \cmidrule{2-11}
\multirow{2}{*}{\small Supervised} & \small \textsc{shmm-me}	& 93.9	& 97.4 	& 95.1	& 95.8 	& 93.8	& 95.5		& 93.8	& 95.5	& 95.1 \\
  & \small \textsc{hsm} 		& 95.2  & 97.4	& 97.4	& 95.2		& 94.5	& 96.0	& 95.6	& 92.2 & 95.3		 \\
\cbottomrule{2-11}
\end{tabular}
\vspace{-\baselineskip}
\caption{Part-of-speech induction results in multiple languages.}\label{tab:pos}
\end{table*}

\section{Boostrapping NER with type-level supervision}\label{sec:ner}

Name lists and dictionaries are useful for NER particularly when in-domain annotations are scarce.
However, with little annotated data, discriminative training may be unable to reliably estimate lexical feature weights and may overfit. In this section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER.

\vspace{5pt}

\subsection{Data}\label{sec:nerdata}

Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations. Therefore, rapidly deploying natural language technologies in a new language may be challenging.
In the context of facilitating relief responses in emergencies such as
natural disasters, the DARPA LORELEI (Low Resource Languages for
Emergent Incidents) program has sponsored the development and release
of representative ``language packs'' for Turkish and Uzbek with more languages planned~\cite{lorelei2016}.
We use the named-entity annotations as
part of these language packs which include persons, locations,
organizations, and geo-political entities, in order to explore bootstrapping
named-entity recognition from small amounts of data. We consider two types of data: \circone in-context annotations, where sentences are fully annotated for named-entities, and \circtwo lexical resources.

The LORELEI language packs lack adequate in-domain lexical resources for our purposes. Therefore, we simulate in-domain lexical resources by holding out portions of the
annotated development data and deriving dictionaries and name lists from them. 
For each label $y \in \{\textsc{per, loc, org, gpe, context}\}$,  our lexicon for $y$
lists all distinct $y$-labeled strings that appear in the held-out data.
This setup ensures that the labels associated with lexicon entries correspond to the annotation guidelines used in the data we use for evaluation.  It avoids possible problems that might arise when leveraging noisy out-of-domain knowledge bases, which we may explore in future.

\subsection{Evaluation}\label{sec:nerexpts}

In this section we report supervised NER experiments on two low-resource languages: Turkish and Uzbek.  We vary both the amount of supervision as well as the size of the lexical resources. A challenge when evaluating the performance of a model with small amounts of training data is that there may be high-variance in the results. In order to have more confidence in our results, we perform bootstrap resampling experiments in which the training set, evaluation set, and lexical resources are randomized across several replications of the same experiment (for each of the data conditions). We use 10 replications for each of the data conditions reported in Figures~\ref{fig:nersup}--\ref{fig:nerdelta}, and report both the mean performance and 95\% confidence intervals.

\vspace{5pt}

\noindent \textbf{Baseline.} We use the Stanford NER
system with a standard set of language-independent features~\cite{Finkel:2005:INI:1219840.1219885}.\footnote{We also experimented with neural models, but found that the CRF outperformed them in low-data conditions.}. This model is a conditional random field (CRF) with feature templates which include character $n$-grams as well as word shape features. Crucially, we also incorporate lexical features. The CRF parameters are regularized using an L1 penalty and optimized via Orthant-wise limited-memory quasi-Newton optimization~\cite{andrew2007scalable}. For both our proposed method and the discriminative baseline, we use a fixed set of hyperparameters (i.e. we do not use a separate validation set for tuning each data condition).
In order to make a fair comparison to the CRF, we use our sampler for forward inference only, without resampling on the test data.

\vspace{5pt}

\noindent \textbf{Results.} We show learning curves as a function of supervised training corpus size.  Figure~\ref{fig:nersup} shows that our generative model strongly beats the baseline in this low-data regime.  In particular, when there is little annotated training data, our proposed generative model can compensate by exploiting the lexicon, while the discriminative baseline scores terribly.  The performance gap decreases with larger supervised corpora, which is consistent with prior results comparing generative and discriminative training~\cite{ng2002discriminative}.

 In Figure~\ref{fig:nerdelta}, we show the effect of the lexicon's size: as expected, larger lexicons are better.  The generative approach significantly outperforms the discriminative baseline at any lexicon size, although its advantage drops for smaller lexicons or larger training corpora.  

In Figure~\ref{fig:nersup} we found that increasing the pseudocount $c$ consistently \emph{decreases} performance, so we used $c=1$ in our other experiments.\footnote{Why?  Even a pseudocount of $c=1$ is enough to ensure that $P_y(s) \gg H_y(s)$, since the prior probability $H_y(s)$ is rather small for most strings in the lexicon.  Indeed, perhaps $c < 1$ would have increased performance, particularly if the lexicon reflects out-of-domain data.  This could be arranged, in effect, by using a hierarchical Bayesian model in which the lexicon and corpus emissions are not drawn from the identical distribution $P_y$ but only from similar (coupled) distributions.}

\begin{figure*}
  \centering
  \includegraphics[width=0.80\textwidth]{plots/turkish_pseudosweep_500_updated.pdf}
  \caption{\textbf{Absolute} NER performance for Turkish ($y$-axis) as a function of corpus size ($x$-axis).  The $y$-axis gives the F1 score on a held-out evaluation set (averaged over 10 bootstrap replicates, with error bars showing 95\% confidence intervals).  Our generative approach is compared to a baseline discriminative model with lexicon features (lowest curve).  500 held-out sentences were used to create the lexicon for both methods.  Note that increasing the pseudocount $c$ for lexicon entries (upper curves) tends to \emph{decrease} performance for the generative model; we therefore take $c=1$ in all other experiments.  This graph shows Turkish; the corresponding Uzbek figure is available as supplementary material.}\label{fig:nersup}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.80\textwidth]{plots/turkish_delta_500.pdf}
  \caption{\textbf{Relative} NER performance for Turkish ($y$-axis) as a function of corpus size ($x$-axis).  In this graph, $c=1$ is constant and the curves instead compare different lexicon sizes derived from 10, 100, and 1000 held-out sentences.  
    The $y$-axis now gives the \emph{difference}
    $\text{F1}_{\text{model}} - \text{F1}_{\text{baseline}}$, so 
    positive values indicate \emph{improvement over the baseline} due to
    the proposed model.  Gains are highest for large lexicons and for small corpora.
    Again, the corresponding Uzbek figure is available as supplementary material.}\label{fig:nerdelta}
\end{figure*}

\section{Conclusion}\label{sec:conclusion}

This paper has described a generative model for low-resource sequence labeling and segmentation tasks using lexical
resources. Experiments in semi-supervised and low-resource settings have demonstrated its applicability to part-of-speech induction and low-resource named-entity recognition.
 There are many potential avenues for future work.
Our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important.
 It would also be interesting to explore more expressive parameterizations,
 such recurrent neural networks for $H_y$. In the space of neural methods, differentiable memory \cite{DBLP:journals/corr/SantoroBBWL16} may be more flexible than the PYP prior, while retaining the ability of the model to cache strings observed in the gazetteer.

\section*{Acknowledgments}

This work was supported by the JHU Human Language Technology
Center of Excellence, DARPA LORELEI, and NSF grant IIS-1423276.
Thanks to Jay Feldman for early discussions.

\bibliography{acl2017}
\bibliographystyle{acl_natbib}

\end{document}
