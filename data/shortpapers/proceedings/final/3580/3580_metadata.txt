SubmissionNumber#=%=#3580
FinalPaperTitle#=%=#Attention Strategies for Multi-Source Sequence-to-Sequence Learning
ShortPaperTitle#=%=#Attention Strategies for Multi-Source Sequence-to-Sequence Learning
NumberOfPages#=%=#7
CopyrightSigned#=%=#Jindřich Helcl
JobTitle#==#
Organization#==#Charles University
Malostranské náměstí 25
118 00 Praha 1
Czech Republic
Abstract#==#Modeling attention in neural multi-source sequence-to-sequence learning remains
a relatively unexplored area, despite its usefulness in tasks that incorporate
multiple source languages or modalities.
We propose two novel approaches to combine the outputs of attention mechanisms
over each source sequence, flat and hierarchical.
We compare the proposed methods with existing techniques and present results of
systematic evaluation of those methods on the WMT16 Multimodal Translation and
Automatic Post-editing tasks.
We show that the proposed methods achieve competitive results on both tasks.
Author{1}{Firstname}#=%=#Jindřich
Author{1}{Lastname}#=%=#Libovický
Author{1}{Email}#=%=#libovicky@ufal.mff.cuni.cz
Author{1}{Affiliation}#=%=#Charles University in Prague
Author{2}{Firstname}#=%=#Jindřich
Author{2}{Lastname}#=%=#Helcl
Author{2}{Email}#=%=#helcl@ufal.mff.cuni.cz
Author{2}{Affiliation}#=%=#Charles University in Prague

==========